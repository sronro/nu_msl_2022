{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd      \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import string\n",
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"./data/outputs/autonomous_clean.pkl\"\n",
    "df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean text. Since text is on web (HTML) format, we can use BeautifulSoup to parse it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup(text):\n",
    "    text = BeautifulSoup(text, \"html5lib\").get_text()\n",
    "    return text\n",
    "df['text'] = df['text'].apply(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize the text, meaning breaking up each sentence to words, and eliminating ends of each word such as -ed, -ize, -ing’s.\n",
    "\n",
    "Patents have certain patent-specific languages that can be repetitive. Such language can interfere with the result, so we want to remove them. We also don’t want words that are so unique that it shows up only once in more than a thousand abstracts. To get only the words that would help my analysis, I entered words to take out (stopwords) and used Regex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_list = [\n",
    "'claims','claim', 'method', 'provide', 'provided', \\\n",
    "'device', 'devices','apparatus','system', 'systems', \\\n",
    "'apparatuses', 'embodiments', 'embodiment','examples', \\\n",
    "'example','inventions', 'invention', 'present', \\\n",
    "'includes', 'include', 'including','description', \\\n",
    "'user', 'body', 'power', 'person', 'persons', \\\n",
    "'comprising', 'comprise', 'comprises', 'configured', \\\n",
    "'configure','for example', 'discloses', 'disclose', \\\n",
    "'method', 'said', 'abstract', 'abstracts', 'disclosed', 'herein', \\\n",
    "'autonomous', 'vehicle', 'self-driving', 'sensor'\n",
    "]\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "#larger list containing all custom stop words as well as from NLP libraries\n",
    "stop = set(list(stop_list) + list(ENGLISH_STOP_WORDS) \\\n",
    "           + stopwords.words('english') + punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex is a programming tool that lets us find certain expressions and apply a global change to text, sort of like a sophisticated control+F change. For example, if we want to remove all words before the word “embodiment” from a sentence, we can write a regex code to identify all sentences with “embodiment,” and remove the word “embodiment” plus all words between “embodiment” and the period immediately before the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeText(text):\n",
    "    #clean text using regex\n",
    "    separators = [\"\\xa0\\xa0\\xa0\\xa0\", \"\\r\", \"\\n\",\\\n",
    "                  \"\\t\", \"'m\", \"'ll\", '^\\d+\\s|\\s\\d+\\s|\\s\\d+$']\n",
    "    for i in separators:\n",
    "        text = re.sub(i, \" \", text.lower())\n",
    "    # Use Spacy to parse text    \n",
    "    doc = nlp(text)\n",
    "    # Lemmatization\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "    # Remove stop words\n",
    "    tokens = [tok for tok in tokens if len(tok) != 1 and tok not in stop]\n",
    "    return tokens\n",
    "\n",
    "def text_processing(corp):\n",
    "    corp = tokenizeText(corp)\n",
    "    return ' '.join(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply text processing to every row in Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57b491026ee43dbb80d733bcbfe9255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['text'] = df['text'].progress_apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "priority date\n",
       "2017-03-20    assembly module external surface receive seal ...\n",
       "2018-07-19    av determine datum road geometry datum plurali...\n",
       "2016-03-15    drive behavior modification receive passenger ...\n",
       "2014-03-18    increase safety comfort driving drive arrangem...\n",
       "2012-03-15    operate mode determine current state current s...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filepath = \"./data/outputs/autonomous_tokenized.pkl\"\n",
    "df.to_pickle(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
